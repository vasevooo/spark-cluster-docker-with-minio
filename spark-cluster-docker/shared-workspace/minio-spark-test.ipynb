{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7227392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "991d1db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PySpark version: 3.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 18:22:53 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using PySpark version: {pyspark.__version__}\") # Should print 3.1.3\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"MyS3App\") \\\n",
    "        .config(\"spark.local.dir\", \"/opt/workspace/spark-tmp\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.buffer.dir\", \"/opt/workspace/s3a-buffer\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ae8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_endpoint_url = os.getenv('MINIO_ENDPOINT_URL', 'http://minio:9000')\n",
    "# Replace fallback values ONLY if not using environment variables\n",
    "minio_access_key = os.getenv('MINIO_ROOT_USER', 'YOUR_MINIO_ACCESS_KEY')\n",
    "minio_secret_key = os.getenv('MINIO_ROOT_PASSWORD', 'YOUR_MINIO_SECRET_KEY')\n",
    "# Use consistent bucket naming\n",
    "bucket_name = os.getenv('MINIO_DEFAULT_BUCKET', 'your-bucket-name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef907a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring S3 client for endpoint: http://minio:9000\n",
      "Ensuring bucket 'test-bucket' exists...\n",
      "Bucket 'test-bucket' already exists.\n",
      "Bucket check/creation process finished.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Configuring S3 client for endpoint: {minio_endpoint_url}\")\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=minio_endpoint_url,\n",
    "    aws_access_key_id=minio_access_key,\n",
    "    aws_secret_access_key=minio_secret_key,\n",
    ")\n",
    "\n",
    "# --- Check and Create Bucket ---\n",
    "print(f\"Ensuring bucket '{bucket_name}' exists...\")\n",
    "try:\n",
    "    # Check if bucket exists. head_bucket throws an exception if it doesn't exist.\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "except ClientError as e:\n",
    "    # Check if the error is specifically a \"Not Found\" or \"NoSuchBucket\" error\n",
    "    error_code = e.response.get('Error', {}).get('Code')\n",
    "    # MinIO might return 404 or NoSuchBucket depending on configuration/version\n",
    "    if error_code == '404' or 'NoSuchBucket' in str(e):\n",
    "        print(f\"Bucket '{bucket_name}' does not exist. Attempting to create...\")\n",
    "        try:\n",
    "            # Create the bucket\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "        except ClientError as creation_error:\n",
    "            print(f\"Error creating bucket '{bucket_name}': {creation_error}\")\n",
    "            # Decide if you want to stop execution if bucket creation fails\n",
    "            raise creation_error\n",
    "    else:\n",
    "        # Handle other potential errors (permissions, network issues, etc.)\n",
    "        print(f\"Error checking bucket status for '{bucket_name}': {e}\")\n",
    "        raise e\n",
    "\n",
    "print(\"Bucket check/creation process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f15bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame created:\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  Alice|  1|\n",
      "|    Bob|  2|\n",
      "|Charlie|  3|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create Sample Data\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "columns = [\"name\", \"id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Sample DataFrame created:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d51761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define S3A Path\n",
    "output_path = f\"s3a://{bucket_name}/test-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57ca97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to write DataFrame to: s3a://test-bucket/test-data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 18:22:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote data to MinIO.\n"
     ]
    }
   ],
   "source": [
    "# 3. Write DataFrame to MinIO\n",
    "print(f\"Attempting to write DataFrame to: {output_path}\")\n",
    "try:\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(\"Successfully wrote data to MinIO.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to MinIO: {e}\")\n",
    "    # Print stack trace for more details if needed\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    spark.stop()\n",
    "    raise e # Re-raise exception to stop the cell execution clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ce061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read data back from: s3a://test-bucket/test-data\n",
      "Successfully read data back from MinIO:\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|Charlie|  3|\n",
      "|  Alice|  1|\n",
      "|    Bob|  2|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Read Data back from MinIO\n",
    "print(f\"Attempting to read data back from: {output_path}\")\n",
    "try:\n",
    "    df_read = spark.read.parquet(output_path)\n",
    "    print(\"Successfully read data back from MinIO:\")\n",
    "    df_read.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from MinIO: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb97717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test finished.\n"
     ]
    }
   ],
   "source": [
    "# 5. Stop SparkSession\n",
    "spark.stop()\n",
    "print(\"Test finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c73340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 17:59:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created.\n",
      "Creating DataFrame from in-memory data...\n",
      "Original DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+----------+\n",
      "|emp_id|first_name|last_name| hire_date|\n",
      "+------+----------+---------+----------+\n",
      "|  1001|     Alice|    Smith|2020-01-15|\n",
      "|  1002|       Bob|  Johnson|2019-05-20|\n",
      "|  1003|   Charlie| Williams|2021-08-01|\n",
      "|  1004|     David|    Brown|2020-01-10|\n",
      "|  1005|       Eve|    Davis|2022-03-25|\n",
      "+------+----------+---------+----------+\n",
      "\n",
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "DataFrame registered as temporary view: 'employees_mem_view'\n",
      "Running Spark SQL query...\n",
      "Query results (Employees hired in 2020):\n",
      "+------+----------+---------+\n",
      "|emp_id|first_name|last_name|\n",
      "+------+----------+---------+\n",
      "|  1001|     Alice|    Smith|\n",
      "|  1004|     David|    Brown|\n",
      "+------+----------+---------+\n",
      "\n",
      "\n",
      "Spark SQL in-memory test finished.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# --- Spark Session ---\n",
    "print(\"Creating SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL_InMemory_Test\") \\\n",
    "    .master(\"spark://jupyter-spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created.\")\n",
    "\n",
    "# --- Create In-Memory Data ---\n",
    "# Sample data representing employees\n",
    "data = [\n",
    "    (1001, \"Alice\", \"Smith\", date(2020, 1, 15)),\n",
    "    (1002, \"Bob\", \"Johnson\", date(2019, 5, 20)),\n",
    "    (1003, \"Charlie\", \"Williams\", date(2021, 8, 1)),\n",
    "    (1004, \"David\", \"Brown\", date(2020, 1, 10)),\n",
    "    (1005, \"Eve\", \"Davis\", date(2022, 3, 25))\n",
    "]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# --- Create DataFrame ---\n",
    "print(\"Creating DataFrame from in-memory data...\")\n",
    "employee_df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "employee_df.show()\n",
    "employee_df.printSchema()\n",
    "\n",
    "# --- Register DataFrame as SQL View ---\n",
    "view_name = \"employees_mem_view\"\n",
    "employee_df.createOrReplaceTempView(view_name)\n",
    "print(f\"DataFrame registered as temporary view: '{view_name}'\")\n",
    "\n",
    "# --- Run Spark SQL Query ---\n",
    "# Example: Select employees hired in 2020\n",
    "print(\"Running Spark SQL query...\")\n",
    "sql_query = f\"\"\"\n",
    "SELECT emp_id, first_name, last_name\n",
    "FROM {view_name}\n",
    "WHERE year(hire_date) = 2020\n",
    "ORDER BY emp_id\n",
    "\"\"\"\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "print(\"Query results (Employees hired in 2020):\")\n",
    "result_df.show()\n",
    "\n",
    "# --- Stop SparkSession ---\n",
    "spark.stop()\n",
    "print(\"\\nSpark SQL in-memory test finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ba97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error writing to MinIO: An error occurred while calling o85.parquet.\n",
    ": org.apache.spark.SparkException: Job aborted.\n",
    "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n",
    "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
    "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
    "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
    "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
    "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
    "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
    "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
    "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
    "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
    "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
    "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
    "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
    "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
    "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
    "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
    "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
    "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
    "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
    "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
    "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
    "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
    "\tat java.lang.Thread.run(Thread.java:750)\n",
    "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 15) (172.19.0.6 executor 0): org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for s3ablock-0001-\n",
    "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:463)\n",
    "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:477)\n",
    "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:213)\n",
    "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:575)\n",
    "\tat org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:811)\n",
    "\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:190)\n",
    "\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:168)\n",
    "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:781)\n",
    "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
    "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
    "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
    "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
    "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
    "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
    "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
    "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\n",
    "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
    "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
    "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
    "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
    "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
    "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
    "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
    "\tat java.lang.Thread.run(Thread.java:750)\n",
    "\n",
    "Driver stacktrace:\n",
    "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2303)\n",
    "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2252)\n",
    "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2251)\n",
    "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
    "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
    "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
    "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2251)\n",
    "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
    "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
    "\tat scala.Option.foreach(Option.scala:407)\n",
    "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
    "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2490)\n",
    "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)\n",
    "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)\n",
    "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
    "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n",
    "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
    "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
    "\t... 33 more\n",
    "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for s3ablock-0001-\n",
    "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:463)\n",
    "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:477)\n",
    "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:213)\n",
    "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:575)\n",
    "\tat org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:811)\n",
    "\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:190)\n",
    "\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:168)\n",
    "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:781)\n",
    "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
    "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
    "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
    "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
    "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
    "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
    "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
    "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\n",
    "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
    "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
    "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
    "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
    "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
    "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
    "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
    "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
    "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
